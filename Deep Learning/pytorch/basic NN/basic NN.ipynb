{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x5b654d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor，叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X: \ttensor([4., 4., 3., 0.]) \n",
      " W_1: \ttensor([[3., 4., 2., 3.],\n",
      "        [2., 3., 1., 1.]], requires_grad=True) \n",
      " Z: \ttensor([34., 23.], grad_fn=<MvBackward>) \n",
      " W_2: \ttensor([1., 4.], requires_grad=True) \n",
      " Y: \ttensor(126., grad_fn=<DotBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为了方便，随机创建一些int型的tensor，然后将它们转换为float型\n",
    "X   = torch.randint(5, (4,)  ).float().requires_grad_(False)\n",
    "W_1 = torch.randint(5, (2, 4)).float().requires_grad_(True)\n",
    "Z   = W_1.matmul(X)\n",
    "W_2 = torch.randint(5, (2,)  ).float().requires_grad_(True)\n",
    "Y   = W_2.matmul(Z)\n",
    "\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X, W_1, Z, W_2, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is leaf?\n",
      " X: \tTrue \n",
      " W_1: \tTrue \n",
      " Z: \tFalse \n",
      " W_2: \tTrue \n",
      " Y: \tFalse \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" is leaf?\")\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X.is_leaf, W_1.is_leaf, Z.is_leaf, W_2.is_leaf, Y.is_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度，反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grad\n",
      " X: \tNone \n",
      " W_1: \ttensor([[ 4.,  4.,  3.,  0.],\n",
      "        [16., 16., 12.,  0.]]) \n",
      " Z: \tNone \n",
      " W_2: \ttensor([34., 23.]) \n",
      " Y: \tNone \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y.backward()\n",
    "print(\" grad\")\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X.grad, W_1.grad, Z.grad, W_2.grad, Y.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grad2\n",
      " Z: \tNone \n",
      " W_2: \ttensor([68., 46.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z.detach_()\n",
    "W_2.detach_().requires_grad_(True)\n",
    "\n",
    "Z_2 = W_2.matmul(Z)\n",
    "Z_2.backward()\n",
    "\n",
    "print(\" grad2\")\n",
    "print(\" Z: \\t%s \\n W_2: \\t%s \\n\" % (Z.grad, W_2.grad))\n",
    "\n",
    "# 可以发现detach_()以后require_grad信息被清除\n",
    "# 但是grad还保留，第二次反向传播后，两次的grad累积了起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X: \ttensor([3., 1., 1., 3.]) \n",
      " W_1: \ttensor([[4., 3., 1., 4.],\n",
      "        [1., 4., 4., 1.]], requires_grad=True) \n",
      " W_2: \ttensor([4., 4.], requires_grad=True) \n",
      "\n",
      " ====================================\n",
      " for epoch 0, Y is tensor(168.)\n",
      " for epoch 1, Y is tensor(152.1360)\n",
      " for epoch 2, Y is tensor(137.5519)\n",
      " for epoch 3, Y is tensor(124.1259)\n",
      " for epoch 4, Y is tensor(111.7461)\n",
      " for epoch 5, Y is tensor(100.3092)\n",
      " for epoch 6, Y is tensor(89.7202)\n",
      " for epoch 7, Y is tensor(79.8908)\n",
      " for epoch 8, Y is tensor(70.7396)\n",
      " for epoch 9, Y is tensor(62.1905)\n"
     ]
    }
   ],
   "source": [
    "# 前向传播，参数是所有的叶子节点，后面的是参数\n",
    "def forward(X, W_1, W_2):\n",
    "    Z = W_1.matmul(X)\n",
    "    Y = W_2.matmul(Z)\n",
    "    return Y\n",
    "\n",
    "# 初始化\n",
    "X   = torch.randint(5, (4,)  ).float().requires_grad_(False)\n",
    "W_1 = torch.randint(5, (2, 4)).float().requires_grad_(True)\n",
    "W_2 = torch.randint(5, (2,)  ).float().requires_grad_(True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n W_2: \\t%s \\n\" % (X, W_1, W_2))\n",
    "print(\" ====================================\")\n",
    "\n",
    "# 更新模块\n",
    "for epoch in range(10):\n",
    "    # 梯度清零\n",
    "    if W_1.grad is not None:\n",
    "        W_1.grad.zero_()\n",
    "    if W_2.grad is not None:\n",
    "        W_2.grad.zero_()\n",
    "    \n",
    "    # 前向传播\n",
    "    Y = forward(X, W_1, W_2)\n",
    "    \n",
    "    # 反向传播\n",
    "    Y.backward()\n",
    "    \n",
    "    # 参数更新（数据域）\n",
    "    W_1 = W_1 - learning_rate * W_1.grad\n",
    "    W_2 = W_2 - learning_rate * W_2.grad\n",
    "    \n",
    "    # 脱离计算图（参数更新过程中产生的）\n",
    "    W_1.detach_().requires_grad_(True)\n",
    "    W_2.detach_().requires_grad_(True)\n",
    "    \n",
    "    print(\" for epoch %d, Y is %s\" % (epoch, Y.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a: tensor([2]) \n",
      " b: tensor([1])\n",
      " \n",
      " top 5 datas:\n",
      " x \t: tensor([15.2550, 46.6000,  8.7955, 13.4917,  7.5340]) \n",
      " noise \t: tensor([-0.0933,  0.6871, -0.8383,  0.0009,  0.8419]) \n",
      " y \t: tensor([31.4167, 94.8871, 17.7527, 27.9842, 16.9099]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据生成\n",
    "# 直线的真实 a 和 b\n",
    "a, b = torch.randint(1, 5, (1, )), torch.randint(1, 5, (1, ))\n",
    "print(\" a: %s \\n b: %s\" % (a, b))\n",
    "# 生成 x\n",
    "x = torch.rand((50, )) * 50\n",
    "# 生成噪声\n",
    "noise = torch.randn((50, ))\n",
    "# 计算 y\n",
    "y = a * x + b + noise\n",
    "\n",
    "print(\" \\n top 5 datas:\")\n",
    "print(\" x \\t: %s \\n noise \\t: %s \\n y \\t: %s \\n\" % (x[0:5], noise[0:5], y[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据数据点画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x, y, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据生成器，用于读取批量数据\n",
    "def data_iter(batch_size, x, y):\n",
    "    # 数据大小\n",
    "    data_num = len(x)\n",
    "    # 随机排列\n",
    "    permutation = torch.randperm(data_num)\n",
    "    # 每次取 batch_size 个\n",
    "    for i in range(0, data_num, batch_size):\n",
    "        # 每个 batch 的下标\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        yield x[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target  A: 2.000000, B: 1.000000\n",
      " initial A: 0.840453, B: 0.496759\n",
      " ====================================\n",
      " for epoch 0\t, Loss is 0.22005189955234528\t A: 2.010662, B: 0.531466\n",
      " for epoch 500\t, Loss is 0.09574466943740845\t A: 1.973767, B: 0.947333\n",
      " for epoch 1000\t, Loss is 0.03607318550348282\t A: 1.985599, B: 1.134936\n",
      " for epoch 1500\t, Loss is 2.4995505809783936\t A: 1.990436, B: 1.220327\n",
      " for epoch 2000\t, Loss is 0.23648767173290253\t A: 1.981476, B: 1.259384\n",
      " for epoch 2500\t, Loss is 0.03790779411792755\t A: 1.974543, B: 1.276665\n",
      " for epoch 3000\t, Loss is 0.6223516464233398\t A: 1.994190, B: 1.285105\n",
      " for epoch 3500\t, Loss is 0.239980548620224\t A: 1.986317, B: 1.287909\n",
      " for epoch 4000\t, Loss is 0.667565107345581\t A: 1.998233, B: 1.291380\n",
      " for epoch 4500\t, Loss is 0.466873437166214\t A: 1.959629, B: 1.289387\n",
      " for epoch 5000\t, Loss is 0.9469639658927917\t A: 1.980472, B: 1.288054\n"
     ]
    }
   ],
   "source": [
    "# 参数初始化\n",
    "A, B = torch.rand(1, requires_grad=True), torch.rand(1, requires_grad=True)\n",
    "print(\" target  A: %f, B: %f\" % (a, b))\n",
    "print(\" initial A: %f, B: %f\" % (A, B))\n",
    "print(\" ====================================\")\n",
    "# 训练阶段\n",
    "epoch_num  = 5001\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for X, Y in data_iter(batch_size, x, y):\n",
    "\n",
    "        # 前向传播，计算损失\n",
    "        Loss = ((A * X + B - Y) ** 2 / 2).sum()\n",
    "\n",
    "        # 反向传播\n",
    "        Loss.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        A = (A - A.grad * learning_rate / batch_size).detach_().requires_grad_()\n",
    "        B = (B - B.grad * learning_rate / batch_size).detach_().requires_grad_()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(\" for epoch %d\\t, Loss is %s\" % (epoch, Loss.item()), end='\\t')\n",
    "        print(\" A: %f, B: %f\" % (A.item(), B.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
