{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x5a02c10>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor，叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X: \ttensor([4., 4., 3., 0.]) \n",
      " W_1: \ttensor([[3., 4., 2., 3.],\n",
      "        [2., 3., 1., 1.]], requires_grad=True) \n",
      " Z: \ttensor([34., 23.], grad_fn=<SqueezeBackward3>) \n",
      " W_2: \ttensor([1., 4.], requires_grad=True) \n",
      " Y: \ttensor(126., grad_fn=<DotBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 为了方便，随机创建一些int型的tensor，然后将它们转换为float型\n",
    "X   = torch.randint(5, (4,)  ).float().requires_grad_(False)\n",
    "W_1 = torch.randint(5, (2, 4)).float().requires_grad_(True)\n",
    "Z   = X.matmul(W_1.T)\n",
    "W_2 = torch.randint(5, (2,)  ).float().requires_grad_(True)\n",
    "Y   = Z.matmul(W_2)\n",
    "\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X, W_1, Z, W_2, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " is leaf?\n",
      " X: \tTrue \n",
      " W_1: \tTrue \n",
      " Z: \tFalse \n",
      " W_2: \tTrue \n",
      " Y: \tFalse \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" is leaf?\")\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X.is_leaf, W_1.is_leaf, Z.is_leaf, W_2.is_leaf, Y.is_leaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度，反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grad\n",
      " X: \tNone \n",
      " W_1: \ttensor([[ 4.,  4.,  3.,  0.],\n",
      "        [16., 16., 12.,  0.]]) \n",
      " Z: \tNone \n",
      " W_2: \ttensor([34., 23.]) \n",
      " Y: \tNone \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y.backward()\n",
    "print(\" grad\")\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n Z: \\t%s \\n W_2: \\t%s \\n Y: \\t%s \\n\" % (X.grad, W_1.grad, Z.grad, W_2.grad, Y.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " grad2\n",
      " Z: \tNone \n",
      " W_2: \ttensor([68., 46.]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Z.detach_()\n",
    "W_2.detach_().requires_grad_(True)\n",
    "\n",
    "Z_2 = Z.matmul(W_2)\n",
    "Z_2.backward()\n",
    "\n",
    "print(\" grad2\")\n",
    "print(\" Z: \\t%s \\n W_2: \\t%s \\n\" % (Z.grad, W_2.grad))\n",
    "\n",
    "# 可以发现detach_()以后require_grad信息被清除\n",
    "# 但是grad还保留，第二次反向传播后，两次的grad累积了起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X: \ttensor([4., 0., 1., 2.]) \n",
      " W_1: \ttensor([[3., 0., 0., 0.],\n",
      "        [2., 4., 1., 3.]], requires_grad=True) \n",
      " W_2: \ttensor([3., 3.], requires_grad=True) \n",
      "\n",
      " ====================================\n",
      " for epoch 0, Y is tensor(81.)\n",
      " for epoch 1, Y is tensor(73.7001)\n",
      " for epoch 2, Y is tensor(67.0496)\n",
      " for epoch 3, Y is tensor(60.9899)\n",
      " for epoch 4, Y is tensor(55.4677)\n",
      " for epoch 5, Y is tensor(50.4343)\n",
      " for epoch 6, Y is tensor(45.8455)\n",
      " for epoch 7, Y is tensor(41.6608)\n",
      " for epoch 8, Y is tensor(37.8434)\n",
      " for epoch 9, Y is tensor(34.3598)\n"
     ]
    }
   ],
   "source": [
    "# 前向传播，参数是所有的叶子节点，后面的是参数\n",
    "def forward(X, W_1, W_2):\n",
    "    Z = X.matmul(W_1.T)\n",
    "    Y = Z.matmul(W_2)\n",
    "    return Y\n",
    "\n",
    "# 初始化\n",
    "X   = torch.randint(5, (4,)  ).float().requires_grad_(False)\n",
    "W_1 = torch.randint(5, (2, 4)).float().requires_grad_(True)\n",
    "W_2 = torch.randint(5, (2,)  ).float().requires_grad_(True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(\" X: \\t%s \\n W_1: \\t%s \\n W_2: \\t%s \\n\" % (X, W_1, W_2))\n",
    "print(\" ====================================\")\n",
    "\n",
    "# 更新模块\n",
    "for epoch in range(10):\n",
    "    # 梯度清零\n",
    "    if W_1.grad is not None:\n",
    "        W_1.grad.zero_()\n",
    "    if W_2.grad is not None:\n",
    "        W_2.grad.zero_()\n",
    "    \n",
    "    # 前向传播\n",
    "    Y = forward(X, W_1, W_2)\n",
    "    \n",
    "    # 反向传播\n",
    "    Y.backward()\n",
    "    \n",
    "    # 参数更新（数据域）\n",
    "    W_1 = W_1 - learning_rate * W_1.grad\n",
    "    W_2 = W_2 - learning_rate * W_2.grad\n",
    "    \n",
    "    # 脱离计算图（参数更新过程中产生的）\n",
    "    W_1.detach_().requires_grad_(True)\n",
    "    W_2.detach_().requires_grad_(True)\n",
    "    \n",
    "    print(\" for epoch %d, Y is %s\" % (epoch, Y.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
