{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x570dfb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {
    "reber.gif": {
     "image/gif": "R0lGODdh6QE4AQAAACwAAAAA6QE4AYcAAAAAADMAAGYAAJkAAMwAAP8AKwAAKzMAK2YAK5kAK8wAK/8AVQAAVTMAVWYAVZkAVcwAVf8AgAAAgDMAgGYAgJkAgMwAgP8AqgAAqjMAqmYAqpkAqswAqv8A1QAA1TMA1WYA1ZkA1cwA1f8A/wAA/zMA/2YA/5kA/8wA//8zAAAzADMzAGYzAJkzAMwzAP8zKwAzKzMzK2YzK5kzK8wzK/8zVQAzVTMzVWYzVZkzVcwzVf8zgAAzgDMzgGYzgJkzgMwzgP8zqgAzqjMzqmYzqpkzqswzqv8z1QAz1TMz1WYz1Zkz1cwz1f8z/wAz/zMz/2Yz/5kz/8wz//9mAABmADNmAGZmAJlmAMxmAP9mKwBmKzNmK2ZmK5lmK8xmK/9mVQBmVTNmVWZmVZlmVcxmVf9mgABmgDNmgGZmgJlmgMxmgP9mqgBmqjNmqmZmqplmqsxmqv9m1QBm1TNm1WZm1Zlm1cxm1f9m/wBm/zNm/2Zm/5lm/8xm//+ZAACZADOZAGaZAJmZAMyZAP+ZKwCZKzOZK2aZK5mZK8yZK/+ZVQCZVTOZVWaZVZmZVcyZVf+ZgACZgDOZgGaZgJmZgMyZgP+ZqgCZqjOZqmaZqpmZqsyZqv+Z1QCZ1TOZ1WaZ1ZmZ1cyZ1f+Z/wCZ/zOZ/2aZ/5mZ/8yZ///MAADMADPMAGbMAJnMAMzMAP/MKwDMKzPMK2bMK5nMK8zMK//MVQDMVTPMVWbMVZnMVczMVf/MgADMgDPMgGbMgJnMgMzMgP/MqgDMqjPMqmbMqpnMqszMqv/M1QDM1TPM1WbM1ZnM1czM1f/M/wDM/zPM/2bM/5nM/8zM////AAD/ADP/AGb/AJn/AMz/AP//KwD/KzP/K2b/K5n/K8z/K///VQD/VTP/VWb/VZn/Vcz/Vf//gAD/gDP/gGb/gJn/gMz/gP//qgD/qjP/qmb/qpn/qsz/qv//1QD/1TP/1Wb/1Zn/1cz/1f///wD//zP//2b//5n//8z///8AAAAAAAAAAAAAAAAIxAD3CRxIsKDBgwgTKlzIsKHDhxAjSpxIsaLFixgzatzIsaPHjyBDihxJsqTJkyhTqlzJsqXLlzBjypxJs6bNmzhz6tzJs6fPn0CDCh1KtKjRo0iTKl3KtKnTp1CjSp1KtarVq1izat3KtavXr2DDih1LtqzZs2jTql3Ltq3bt3Djyp1Lt67du3jz6t3Lt6/fv4ADCx5MuLDhw4gT7wPAuPFiAIojS/bpWGBjyJMza55ZeWDnzaBDo/xsGbPo06hBXi7IOLWt69caL5uGTbv2RNmtbevezRD3bN7AgfsOTjy47OLIX/9+vDy588zLST+fHjl36ebUsxv2jd0zd+vawwc+ThC3+PN7yStcjb79XOkL2bufvxZ+Q/v0838Ff5u//v9c+ddfdwAWKJWAFOFn4IJMIViRgwxGiBSECRIo4YVBUfighRh2uFNuHEakoYckytSadSFCNGKJLLZ0ImYrqphiizSOhuKMD8VY444jdaZjjjyrBumSjzjmWKSQSGaEompHJulkhZ4x+eSUIi350Y9UZunQi6pp6SVHXF755ZhKPtYlmWhWiOWWabYp4pr3NekmmXD2JuecX9a53p14eqlnQn/2mSWfgApqaKEdBXroor0x6uiVij4qKXNqsqfepGkqaOdnkWJKoqZxgkeopyyCumVlo5JqYHepenepiaqe9l2nB73q4qyxSjarrRvJd+uutObKFrC8KmlqlcQe/yvsXckWe5GzJDXr67J+SQttgsy1+qa1wVIrFrfXinidtkaC2623XpkbLpuukrupb0YMRAuu6Oal7rRQlqcsRt9BsAtjR+hDyHf14gWsEf8CaxF5w0VLbALRBENvwXTNGodA5RBLk5zJQjwvwRS/910cwQgQcbKc4QsosQGcMo8Ku4YsF7AmJ6zwxubtabFAH8csM1w0n7IOyjg1zFqyF/fM3c9A71oz0R/mnO2uAYSSD8wgM90WzSff6/XXGk+9axz6UDGx1vU5HY3NPhctdbtUh/Ly2WinFfTQYeMM7awBgGI2APEqbXTddqvNNt1Driu2bCYLVPbNhKultsSQxz+0JtjnRt7UyAYJrnjUYGs+bOhJYS562l4vRfrpkt+7ueusj26uU7DH3jq4T9Vuu1kgSjuVtaXtzvtszVZVvHf/wn8rYOVUMW9m8l3trTJW0rsLPWXTp4sdqPtef9TgyiNkavfeC7V0WQ4eS3752EOWvfYrr5g5+78GP39R6b9/NP2lo+q+9RMi0Oeexz+jmKZ3w1vPYhYoowL2L3jos5D7CHgqBxrwgGeB0IkoWEELmu+AAPyeBCd4uRB6UEoQJIuGYGSZfJ0wavpKYHxayEEjbWg19+NfDh8YHxbW8FRNItIOyzfEANqJhnXqHmnW98JambBBIcIgDdUko+g8kYhXVAqWtKWsaRUxVs4TXxa1eCRy6WiAyesdjEb0RRHeJlEzAp8DQQjCI7rlch4549sKSMfy9PAtcAKgHtG4O07tEwmQdwrhjwhpO/WNUXWJDAmFrPjaSGqNr5I8xJYkBWgQJrKOe5jM5BuRtTJ9hVJYmmqjKEdJSifu8YQKUmXp+OWwUxIOP7I04rNK4kkL2ieXbqQlL4HJNPgQ84JlMkkv2Te4Y+KvVZh05i1tucoHpUSaWsPmUDp1Sm2GzJsZMuNKwIkucgKFVtQ0JyqpWc1dskSdqoIn9hKVuCb+sS7dYmcK7bk/fAoSJvJkVEB7ci59XoefyLNL5gx6UH4OlKCKNNFD2zRRGJ7JchUdU0Zzcj+GxtCPzPSoLi8qUZB6b6McvaJIP/rD2KE0pT26yY3/GIQ4iOplhyvtZ0DD2LQ+njOnyIwWTFuaMuK9tCacyuhR3cZLpNZxoEsE6geNKlUxpueRQCUSUTEaHWaJyplRrOozlSlTNcZRJT6aYsVuZMu2sVR5DMveF8WatU7WNI9WMpgca1m5pfbIV5McjU72erxN/k+sUx2XjY7nV76a1KonQey4olpYSDEyg2zF6uxu58NSCpYnl8Idk6SoV6oOU7ScTWUoJXs0swLPslGy11ezmDrUTbKbrd0rWk2HV9KyFnSm/GttOXvPz7bWVRL9Wm/tqlC28lVdshNgW10ZXK7qLjb8aexL7gqmWUFMIJSrK2Z/mU7qIhegvDUWdxUT/8fveAwACdjH4S57lcM2B5j3na11cQOBgnhOf0MCTPV807K5AYBsfztfBhXbz3Gat6GJ+84D9jEwqDl1mePF16xMFl4ICGy96TLlb4iZX59ud1YeVkRlbaLb5saVO+9lzAOi8V8M527EWnWRK+kLx+9Y4MMWHuwrSwus/vYMYroAsVZejEODlris/QJy3n7S4t1i7spYbkwF9qFixsRYwXEB64P1xp0JyyvLaL6yi5vMnf4mWcZcVjJmATrmoh5sH+EFM+gAnNrsundtjXmAPrqs5zsy9MksblYcDFzoC/O5z/Ej8CnY0ZhFY63RabNcnZNLLATLeZw8jiAQfWNkABLMuMbaBTWsRGzjHlPNZZceckneoTMrM+8D1ane7ap/yh04VBgIt/60hNJraM5skzuNE0iFxUuj1SHS2FO97o6G+2xoRxu1SJJ2pjf2zM0mSdutQ2q3uTUo321V1OK+4IqnVNlcD5PFIgyylpzn7lam+9hrFPa39VdvknL72rn9rX76DceiAbxuBMcuR3n9aEsK/JqIfTimf5bwhUlW4I5peL0q7kKZAtdMHKdPyBs42JQicOQif/gmQVtyCKI85fD7kE3PPc0AYRy0pE2jyrtLUIvSXHMvb7DMh/pz0QV9n0Mv61tPunOLU5mpTXz5Rqt6yfRPNV0iU080QpEOxQw5desJfZ35cAb2pbcz6egtu9BHes5Zq93sYx172t/OXLa3/cR0h2zct4n3vOv92Pirp989+0ED3mrwxf2p4VWN+MTPPPC6Lvt6s/69yL9diId8POQje/U+JVW6OJ8l5wf/ecdrfUKjJ71+TZ8yLSqz84YqvR1Pj3qH2VU9Gje6cztIdtf/VYxR9WWVndh736880qzx4DK92e8n3ja7yg8UNgluQjbmeI6KyuVMG4TCd8VW+NDs5hppB6kqfv8f+1w0akyL7vWCV3GC4M894Ts6fvLzfFvwb7wNud6r/4W99go3IFXnefUnJs/TfP13Q7C3cQXIcwjEfcbyLAM4J3XEfuXSgMbndBKof+ZnP9bjfxU3KpGygN7iPxbofRxHKBOYTRnXapNFgjO0IRzIVdnSfy7ocR03gycWau3yOzmogztoY/Ind28ChInGPT0Eg+xCckZ4hDsmRyHnVn/XhBfGahOTgvL2f1SodItDL0oIfFm4guhnbjf2Wlq4hSZHbrTjbWLIR+A2brjThjqkXGREh2iIc9wRLwJBacwGh952h0LGHUCQMfAVDXw4cQyHbNHwOPoGiKDWaYw4fAEdNmrcEQfLkA8JhoiOGIQHE4mS+E7vU2T4oAaeqIn/mwiKLCM3sSZrnCYdTgMNhGAEpfiJpwhxxFJqjfgrljIrRkBpQYCJPFWLuuhdhhiMabgrEDMwspiJpiiMi7Vh0cBoaTaNmFM1lAMEs8iKzmhl31E1VzNle9aC8MKIRwCMUriNw4gbVZONtJiOOOYboXAQh9iM6PhcuOFpWbhfrEIs2MiM7ViPyOIb30UQ7BhxKUIsy3iOAPmIdqhus/KL/siDC6le1CaBuHeDAQddE8mJf0iROBaQb7iRz4htNjhi9miGIsmRZNhd0HeS65aSDImSrnaGlqWGMHlNT2iMFHl+LtmHN2l7OemTPcaTPamNPzlaOiOUM5lOCnmUvxcqLrKEQ5zolKgYgKMlh1RZg1bZW1+YlVDCTc6HlU4JIvR0fF6ZFQ+YgJJ0lmhZgWqSyZalRZPuBJcu9lhvSZczc333h5dhtkH8l0x82TRTtFCBiU9EuZWFCTR/yS9dmZg2spgb6JiCqVZ7KZnFZm+WuTUq1ZiZCZRr2ZnVVn6geZkGOJp31H2mOTqlmZqy436sqZnpx5mvKVzqNZuTqYG2GRaQoydimZsf5T+mhXWy6ZsLc141xGdDSJxQYWIMdHvbk5xLytk8opKU9BidNqeC1tma2SkYGLmdETSc3llU4SlgvTmesGWefdGd6Bk9Rrme7vme8Bmf8jmf9Fmf9nmf+Jmf+rmf/Nmf/vmfchEQADs="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reber文法\n",
    "![reber.gif](attachment:reber.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reber文法可以用来验证一个模型是否具有记忆能力。\n",
    "我们可以训练一个模型来判断一个字符串是否是符合reber文法的。  \n",
    "对于RNN来说，训练阶段首先在每次输入一个字符后，都给出下一个字符的监督信号。  \n",
    "测试阶段，对于每一个输入字符后的输出值，判断它是否是满足reber文法的，如果全部满足，则视为成功。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BTXXTTTVVE', 'BPVPXTVPSE', 'BPTVPXTVVE', 'BPTVPXVPSE', 'BPTVPXVPSE', 'BPTTTTVPSE', 'BTXXTTTVVE', 'BPTVPXVPSE', 'BPTTTTTVVE', 'BPTTTTTVVE', 'BTSSSSSXSE', 'BPTVPXVPSE', 'BTXXTTVPSE', 'BPTVPXTVVE', 'BPTTTTVPSE', 'BPTVPXVPSE', 'BPTTTTTVVE', 'BPTTTTVPSE', 'BPTTVPXVVE', 'BTSSSSSXSE', 'BTXXVPXVVE', 'BPTTTTVPSE', 'BPTTTTTVVE', 'BTSSXXTVVE', 'BPVPXTTVVE', 'BTSSSXXVVE', 'BPTTTTTVVE', 'BTXXTTTVVE', 'BPTVPXTVVE', 'BPVPXTTVVE', 'BPTVPXVPSE', 'BTXXTTTVVE', 'BPTVPXVPSE', 'BPTTTTTVVE', 'BPTTTTVPSE', 'BTSSSXXVVE', 'BPTVPXTVVE', 'BPTTTTVPSE', 'BPTTTTTVVE', 'BPTVPXVPSE', 'BPVPXTVPSE', 'BTSSXXTVVE', 'BTSXXTVPSE', 'BTXXVPXVVE', 'BTXXTTVPSE', 'BTSXXTTVVE', 'BTSSSXXVVE', 'BTSSSSSXSE', 'BPTTTTTVVE', 'BPVPXTTVVE', 'BTXXTTVPSE', 'BTXXTTTVVE', 'BTSSSXXVVE', 'BPTTTTVPSE', 'BTSXXTVPSE', 'BTXXVPXVVE', 'BPTTTTVPSE', 'BPTTVPXVVE', 'BTSSXXVPSE', 'BTXXTTVPSE', 'BTSSXXTVVE', 'BTSXXTTVVE', 'BPTTVPXVVE', 'BTSSSXXVVE', 'BTXXVPXVVE', 'BTXXVPXVVE', 'BPVPXTVPSE', 'BPTVPXVPSE', 'BPVPXTTVVE', 'BTSSSSSXSE', 'BTSXXTTVVE', 'BPVPXTTVVE', 'BTXXVPXVVE', 'BTXXTTVPSE', 'BTXXTTTVVE', 'BTSXXTVPSE', 'BPTTTTVPSE', 'BTXXTTVPSE', 'BTXXVPXVVE', 'BPTVPXVPSE', 'BPTTTTVPSE', 'BTSXXTVPSE', 'BPTVPXVPSE', 'BPVPXTTVVE', 'BTSXXTTVVE', 'BPTVPXTVVE', 'BPTVPXTVVE', 'BPTTVPXVVE', 'BPVPXTTVVE', 'BTSSSSSXSE', 'BTXXTTVPSE', 'BPTVPXTVVE', 'BPTVPXTVVE', 'BPTVPXVPSE', 'BPTVPXTVVE', 'BTSSXXVPSE', 'BPTVPXTVVE', 'BTSSSXXVVE', 'BTSSXXVPSE', 'BPTTTTTVVE', 'BPTTTTTVVE', 'BTSXXTTVVE', 'BPTVPXVPSE', 'BTXXTTTVVE', 'BTXXTTVPSE', 'BTSXXTVPSE', 'BPTVPXTVVE', 'BPTVPXTVVE', 'BPTVPXVPSE', 'BTSXXTTVVE', 'BPTTVPXVVE', 'BTSSXXTVVE', 'BTSXXTTVVE', 'BTXXTTTVVE', 'BTSSXXTVVE', 'BTXXTTTVVE', 'BTSSXXVPSE', 'BPTVPXVPSE', 'BTXXVPXVVE', 'BTSSXXVPSE', 'BTSSSXXVVE', 'BTSSSXXVVE', 'BPTTTTTVVE', 'BTSSXXVPSE', 'BPVPXTTVVE', 'BPVPXTTVVE', 'BTSXXTVPSE', 'BTSXXTVPSE', 'BTXXTTTVVE', 'BPVPXTVPSE', 'BPTTTTVPSE', 'BPVPXTTVVE', 'BTSSXXVPSE', 'BPVPXTTVVE', 'BPTVPXVPSE', 'BTSXXTTVVE', 'BTSSSXXVVE', 'BPTTVPXVVE', 'BPTTTTVPSE', 'BPVPXTVPSE', 'BTSXXTVPSE', 'BTXXTTTVVE', 'BTXXTTVPSE', 'BPVPXTTVVE', 'BPTTTTTVVE', 'BTSXXTVPSE', 'BPVPXTVPSE', 'BPTTTTTVVE', 'BPTTTTVPSE', 'BTSSXXTVVE', 'BTSXXTTVVE', 'BTSSSSSXSE', 'BPVPXTVPSE', 'BTSXXTTVVE', 'BTSSXXVPSE', 'BPVPXTTVVE', 'BTSXXTTVVE', 'BPTVPXTVVE', 'BPTTTTVPSE', 'BTXXTTVPSE', 'BPTTTTTVVE', 'BPTTVPXVVE', 'BTXXTTTVVE', 'BPTTTTVPSE', 'BTSSSSSXSE', 'BTSSSXXVVE', 'BTSXXTTVVE', 'BTXXTTTVVE', 'BTXXTTTVVE', 'BPVPXTTVVE', 'BTSSXXVPSE', 'BTSSSXXVVE', 'BPTVPXVPSE', 'BTSSSXXVVE', 'BTSXXTTVVE', 'BPTTTTVPSE', 'BPTVPXTVVE', 'BTSXXTTVVE', 'BTXXTTTVVE', 'BPTVPXVPSE', 'BTXXTTVPSE', 'BTSSXXTVVE', 'BPTVPXTVVE', 'BTSSSSSXSE', 'BTSSSSSXSE', 'BTSSXXVPSE', 'BTSSSXXVVE', 'BTXXVPXVVE', 'BTSSSXXVVE', 'BTXXVPXVVE', 'BPTTVPXVVE', 'BTSSSSSXSE', 'BTSSXXVPSE', 'BTSXXTTVVE', 'BPTVPXVPSE', 'BTSXXTVPSE', 'BPVPXTTVVE', 'BPVPXTVPSE', 'BTSXXTVPSE', 'BPTVPXTVVE', 'BTXXTTTVVE', 'BPTVPXTVVE', 'BTSXXTVPSE', 'BTSSXXVPSE', 'BTXXVPXVVE', 'BTXXTTTVVE', 'BPTTTTTVVE', 'BTXXTTTVVE', 'BTSSSXXVVE', 'BPTVPXTVVE', 'BPVPXTTVVE', 'BTXXTTTVVE', 'BPTTVPXVVE', 'BTSSSXXVVE', 'BTXXVPXVVE', 'BPTVPXVPSE', 'BTXXVPXVVE', 'BTXXTTVPSE', 'BPVPXTVPSE', 'BTSSXXVPSE', 'BPTVPXVPSE', 'BPVPXTVPSE', 'BTXXVPXVVE', 'BTSSSXXVVE', 'BPVPXTVPSE', 'BTSXXTTVVE', 'BPTVPXTVVE', 'BTSSSSSXSE', 'BPVPXTVPSE', 'BTSSXXVPSE', 'BTSXXTTVVE', 'BTSSXXVPSE', 'BTSSSXXVVE', 'BPVPXTTVVE', 'BTXXVPXVVE', 'BTSSXXTVVE', 'BPTTTTTVVE', 'BPVPXTVPSE', 'BPTVPXTVVE', 'BTXXVPXVVE', 'BPVPXTTVVE', 'BPTTTTTVVE', 'BPTTTTVPSE', 'BTSXXTVPSE', 'BPTVPXTVVE', 'BTSSXXTVVE', 'BPTTTTVPSE', 'BPVPXTVPSE', 'BTXXTTVPSE', 'BTSXXTTVVE', 'BTSSXXVPSE', 'BPVPXTVPSE', 'BTSXXTVPSE', 'BTXXVPXVVE', 'BTXXTTVPSE', 'BTSXXTTVVE']\n"
     ]
    }
   ],
   "source": [
    "# 生成reber语言\n",
    "def REBER(n, lenth=None):\n",
    "    N = 0\n",
    "    R = []\n",
    "    while N < n:\n",
    "        r, s = \"B\", 0 # 初始文字，状态\n",
    "        transfer_map = {\n",
    "            0 : [(\"T\", 1), (\"P\", 2)],\n",
    "            1 : [(\"S\", 1), (\"X\", 3)],\n",
    "            2 : [(\"T\", 2), (\"V\", 4)],\n",
    "            3 : [(\"X\", 2), (\"S\", 5)],\n",
    "            4 : [(\"P\", 3), (\"V\", 5)]\n",
    "        }\n",
    "\n",
    "        while s != 5 :\n",
    "            if np.random.rand() > 0.5:\n",
    "                next_state = transfer_map[s][0]\n",
    "            else:\n",
    "                next_state = transfer_map[s][1]\n",
    "            r += next_state[0]\n",
    "            s = next_state[1]\n",
    "\n",
    "        r += \"E\"\n",
    "        \n",
    "        if lenth:\n",
    "            if len(r) == lenth:\n",
    "                R.append(r)\n",
    "                N += 1\n",
    "        else:\n",
    "            R.append(r)\n",
    "            N += 1\n",
    "    \n",
    "    return R\n",
    "\n",
    "train_data = REBER(256, 10)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B': 0, 'T': 1, 'P': 2, 'S': 3, 'X': 4, 'V': 5, 'E': 6}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_list = list(\"BTPSXVE\")\n",
    "char_num  = len(char_list)\n",
    "char_map  = {c : char_list.index(c) for c in char_list}\n",
    "char_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data  = [1, 2]\n",
    "# index = [0, 1]\n",
    "# data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  = torch.tensor([1, 2])\n",
    "index = torch.tensor([1, 0])\n",
    "data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 4, 2, 5, 1, 0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation = np.arange(7)\n",
    "np.random.shuffle(permutation)\n",
    "permutation.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 1, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 1, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 1, 0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0],\n",
      "         [0, 0, 0, 0, 1, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0]],\n",
      "\n",
      "        [[0, 0, 0, 1, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0]]]), tensor([[2, 2, 1, 1, 1],\n",
      "        [1, 5, 3, 4, 3],\n",
      "        [1, 2, 3, 4, 3],\n",
      "        [1, 4, 3, 5, 3],\n",
      "        [1, 1, 4, 2, 4],\n",
      "        [5, 1, 4, 4, 4],\n",
      "        [2, 5, 5, 5, 5],\n",
      "        [3, 5, 5, 5, 5],\n",
      "        [6, 6, 6, 6, 6]]))\n"
     ]
    }
   ],
   "source": [
    "# 数据生成器，用于读取批量数据\n",
    "# 每个序列的前 L-1 位作为输入，后 L-1个作为监督信号\n",
    "def data_iter(batch_size, data):\n",
    "    # 数据大小\n",
    "    data_num = len(data)\n",
    "    seq_num  = len(data[0])\n",
    "    char_num = 7\n",
    "    # 随机排列\n",
    "    permutation = np.arange(data_num)\n",
    "    np.random.shuffle(permutation)\n",
    "    permutation = permutation.tolist()\n",
    "    # 每次取 batch_size 个\n",
    "    for i in range(0, data_num, batch_size):\n",
    "        # 每个 batch 的下标\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \n",
    "        # 按照时间顺序变换\n",
    "        batch_n_list = [] # 得到每一个字符对应的数字\n",
    "        batch_n_advisor_list = [] # 监督信号\n",
    "        for j in indices:\n",
    "            # 对batch中下标为j的字符串数字化\n",
    "            n_list = []\n",
    "            for k in data[j]:\n",
    "                # 获取该字符对应的数字\n",
    "                n_list.append(char_map[k])\n",
    "            \n",
    "            batch_n_list.append(n_list[:-1])\n",
    "            batch_n_advisor_list.append(n_list[1:])\n",
    "\n",
    "        # 将其传入one_hot函数，得到(batch*seq*char)的tensor\n",
    "        batch_seq_one_hot = F.one_hot(torch.tensor(batch_n_list), char_num)\n",
    "        \n",
    "        # 改为seq first的tensor，转置前两维\n",
    "        seq_batch_one_hot = batch_seq_one_hot.transpose(0, 1)\n",
    "        \n",
    "        # advisor的tensor(batch*seq*char)\n",
    "        batch_seq_advisor = torch.tensor(batch_n_advisor_list)\n",
    "        seq_batch_advisor = batch_seq_advisor.transpose(0, 1) # 转置\n",
    "        \n",
    "        yield seq_batch_one_hot, seq_batch_advisor\n",
    "    \n",
    "data_iter_ = data_iter(5, train_data)\n",
    "for _ in range(1):\n",
    "    data = next(data_iter_)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReberRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # parameters\n",
    "        seq_num  = 9\n",
    "        char_num = 7\n",
    "        \n",
    "        # layers\n",
    "        self.input_hidden_layer = nn.RNN(input_size=char_num, hidden_size=10, bias=False)\n",
    "        self.output_layer = nn.Linear(in_features=10, out_features=char_num, bias=False)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden_output, h_n = self.input_hidden_layer(input)\n",
    "        output = self.output_layer(hidden_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for epoch 0\t\t, Loss is 1.9831829071044922\n",
      " for epoch 2000\t\t, Loss is 0.7491315007209778\n",
      " for epoch 4000\t\t, Loss is 0.45513254404067993\n",
      " for epoch 6000\t\t, Loss is 0.3873934745788574\n",
      " for epoch 8000\t\t, Loss is 0.39099010825157166\n",
      " for epoch 10000\t\t, Loss is 0.37793800234794617\n",
      " for epoch 12000\t\t, Loss is 0.33781343698501587\n",
      " for epoch 14000\t\t, Loss is 0.3482116758823395\n",
      " for epoch 16000\t\t, Loss is 0.3533514738082886\n",
      " for epoch 18000\t\t, Loss is 0.32426369190216064\n"
     ]
    }
   ],
   "source": [
    "myNN = ReberRNN()\n",
    "\n",
    "# 训练阶段\n",
    "epoch_num  = 20000\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# 生成优化器\n",
    "optimizer = optim.SGD(myNN.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for INPUT, TARGET in data_iter(batch_size, train_data):\n",
    "        # forward\n",
    "        output = myNN(INPUT.float())\n",
    "        Loss = F.cross_entropy(output.reshape(-1, 7), TARGET.reshape(-1))\n",
    "        \n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 2000 == 0:\n",
    "        print(\" for epoch %d\\t\\t, Loss is %s\" % (epoch, Loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测一个序列\n",
    "seq = 'BTXXTTTVVE'\n",
    "# 转为 seq_one_hot\n",
    "n_list = []\n",
    "for i in seq:\n",
    "    # 获取该字符对应的数字\n",
    "    n_list.append(char_map[i])\n",
    "seq_one_hot = F.one_hot(torch.tensor(n_list))\n",
    "seq_one_hot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0221,  5.0124,  4.8704, -4.4831, -1.1977, -1.2870, -2.3018]],\n",
       "\n",
       "        [[-1.6839, -0.2463, -3.3010,  4.7795,  4.2166, -0.7522, -2.8991]],\n",
       "\n",
       "        [[-1.9538,  1.4334, -4.0466,  2.5301,  6.7014,  0.7268, -5.0841]],\n",
       "\n",
       "        [[-1.7492,  5.5589, -0.6252, -3.6121,  1.1580,  4.7948, -6.3426]],\n",
       "\n",
       "        [[-1.6583,  6.6383,  0.5184, -3.8122, -0.3796,  2.3790, -4.3550]],\n",
       "\n",
       "        [[-1.9074,  5.3163, -0.7433, -3.4456, -0.1438,  4.8603, -4.3336]],\n",
       "\n",
       "        [[-2.1524,  1.7913,  1.0183, -4.2614, -3.2933,  8.5245, -2.9003]],\n",
       "\n",
       "        [[-0.5383, -3.2413,  0.7687, -2.1167, -6.2600,  7.8797,  1.9091]],\n",
       "\n",
       "        [[ 0.5840, -7.1232,  1.0166,  1.0944, -2.4306, -1.9987,  7.6503]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = myNN(seq_one_hot[:-1].unsqueeze(1).float())\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
